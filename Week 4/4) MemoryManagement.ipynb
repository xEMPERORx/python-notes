{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management \n",
    "Memory management is the process of controlling and coordinating computer memory, assigning blocks to various running programs to optimize overall system performance. It involves the allocation and deallocation of memory blocks to programs as needed.\n",
    "\n",
    "Garbage collection is a form of automatic memory management. The garbage collector attempts to reclaim memory occupied by objects that are no longer in use by the program. This helps in preventing memory leaks and optimizing the available memory.\n",
    "\n",
    "### How Garbage Collection Works\n",
    "\n",
    "1. **Marking**: The garbage collector identifies which objects are still in use and which are not. It traverses the object graph starting from the root objects (e.g., global variables, stack variables) and marks all reachable objects.\n",
    "2. **Sweeping**: The garbage collector then scans the heap for unmarked objects and reclaims their memory.\n",
    "3. **Compacting**: Some garbage collectors also compact the memory by moving live objects together to reduce fragmentation.\n",
    "\n",
    "### Example of Allocation and Deallocation\n",
    "\n",
    "Consider a simple example in Python, which uses automatic garbage collection:\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "def create_linked_list():\n",
    "    head = Node(1)\n",
    "    second = Node(2)\n",
    "    third = Node(3)\n",
    "    \n",
    "    head.next = second\n",
    "    second.next = third\n",
    "    \n",
    "    return head\n",
    "\n",
    "# Allocation\n",
    "linked_list = create_linked_list()\n",
    "\n",
    "# Deallocation\n",
    "# When `linked_list` goes out of scope or is set to None, the garbage collector will reclaim the memory.\n",
    "linked_list = None\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- Memory is allocated when new `Node` objects are created.\n",
    "- When `linked_list` is set to `None`, the reference to the linked list is removed. The garbage collector will then reclaim the memory used by the `Node` objects since they are no longer reachable.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Memory management and garbage collection are crucial for optimizing system performance and preventing memory leaks. By automatically reclaiming memory that is no longer in use, garbage collection helps in maintaining the efficiency and stability of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Counting in Memory Management\n",
    "\n",
    "## What is Reference Counting?\n",
    "\n",
    "Reference counting is a technique used in memory management to keep track of how many references (or pointers) exist to a particular resource, such as an object in memory. When the reference count of an object drops to zero, it means that the object is no longer in use and can be safely deallocated or garbage collected.\n",
    "\n",
    "## How Reference Counting Works\n",
    "\n",
    "1. **Initialization**: When an object is created, its reference count is initialized to one.\n",
    "2. **Incrementing**: Every time a new reference to the object is created, the reference count is incremented.\n",
    "3. **Decrementing**: Every time a reference to the object is destroyed or goes out of scope, the reference count is decremented.\n",
    "4. **Deallocation**: When the reference count reaches zero, the object is deallocated, and its memory is freed.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the following example in Python:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "class MyClass:\n",
    "    pass\n",
    "\n",
    "obj = MyClass()\n",
    "print(sys.getrefcount(obj))  # Output: 2 (one from 'obj' and one from getrefcount)\n",
    "\n",
    "another_ref = obj\n",
    "print(sys.getrefcount(obj))  # Output: 3 (one from 'obj', one from 'another_ref', and one from getrefcount)\n",
    "\n",
    "del another_ref\n",
    "print(sys.getrefcount(obj))  # Output: 2 (one from 'obj' and one from getrefcount)\n",
    "\n",
    "del obj\n",
    "# Now the reference count is 0, and the object is deallocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "a = []\n",
    "print(sys.getrefcount(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "b = a\n",
    "print(sys.getrefcount(b)) \n",
    "print(sys.getrefcount(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "del b\n",
    "print(sys.getrefcount(a)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage Collection and Manual Usage\n",
    "\n",
    "Garbage collection is a form of automatic memory management. The garbage collector attempts to reclaim memory occupied by objects that are no longer in use by the program. This helps in preventing memory leaks and optimizing the available memory.\n",
    "\n",
    "#### How Garbage Collection Works\n",
    "\n",
    "1. **Marking**: The garbage collector identifies which objects are still in use and which are not. It traverses the object graph starting from the root objects (e.g., global variables, stack variables) and marks all reachable objects.\n",
    "2. **Sweeping**: The garbage collector then scans the heap for unmarked objects and reclaims their memory.\n",
    "3. **Compacting**: Some garbage collectors also compact the memory by moving live objects together to reduce fragmentation.\n",
    "\n",
    "#### Manual Garbage Collection in Python\n",
    "\n",
    "In Python, the garbage collection module `gc` provides an interface to the garbage collector. You can use this module to interact with the garbage collector manually.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "import gc\n",
    "\n",
    "# Disable automatic garbage collection\n",
    "gc.disable()\n",
    "\n",
    "# Create some objects\n",
    "a = []\n",
    "b = [a]\n",
    "c = [b]\n",
    "\n",
    "# Manually trigger garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Enable automatic garbage collection again\n",
    "gc.enable()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We disable the automatic garbage collection using `gc.disable()`.\n",
    "- We create some objects.\n",
    "- We manually trigger garbage collection using `gc.collect()`.\n",
    "- Finally, we enable the automatic garbage collection again using `gc.enable()`.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Garbage collection is crucial for optimizing system performance and preventing memory leaks. While Python handles garbage collection automatically, the `gc` module allows for manual control when needed, providing flexibility for advanced memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Enable automatic garbage collection again\n",
    "gc.enable()\n",
    "# Disable automatic garbage collection\n",
    "gc.disable()\n",
    "\n",
    "# Create some objects\n",
    "a = []\n",
    "b = [a]\n",
    "c = [b]\n",
    "\n",
    "# Manually trigger garbage collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'collections': 237, 'collected': 1363, 'uncollectable': 0}, {'collections': 21, 'collected': 461, 'uncollectable': 0}, {'collections': 5, 'collected': 98, 'uncollectable': 0}]\n"
     ]
    }
   ],
   "source": [
    "# get garbage collection stats \n",
    "print(gc.get_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# get unreachable objects \n",
    "print(gc.garbage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Memory Management\n",
    "\n",
    "1. **Avoid Memory Leaks**: Ensure that objects are properly deallocated when they are no longer needed. This can be achieved by setting references to `None` or using context managers to manage resources.\n",
    "\n",
    "2. **Use Generators and Iterators**: Generators and iterators can help reduce memory usage by generating items on-the-fly instead of storing them in memory.\n",
    "\n",
    "3. **Limit the Scope of Variables**: Declare variables in the smallest scope possible to ensure they are deallocated when they go out of scope.\n",
    "\n",
    "4. **Use Built-in Data Structures Efficiently**: Choose the appropriate data structure for your needs. For example, use `set` for membership tests and `deque` for fast appends and pops from both ends.\n",
    "\n",
    "5. **Profile and Monitor Memory Usage**: Use tools like `memory_profiler` and `tracemalloc` to profile and monitor memory usage in your application.\n",
    "\n",
    "6. **Optimize Object Creation**: Reuse objects when possible instead of creating new ones. This can be done by using object pools or caching.\n",
    "\n",
    "7. **Manual Garbage Collection**: In some cases, manually triggering garbage collection using the `gc` module can help manage memory more effectively.\n",
    "\n",
    "8. **Avoid Circular References**: Circular references can prevent the garbage collector from reclaiming memory. Use weak references (`weakref` module) to avoid this issue.\n",
    "\n",
    "9. **Use Immutable Data Structures**: Immutable data structures, such as tuples and frozensets, can help reduce memory usage and improve performance.\n",
    "\n",
    "10. **Be Cautious with Large Data**: When working with large datasets, consider using memory-mapped files or out-of-core processing techniques to avoid loading the entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object obj1 created\n",
      "Object obj2 created\n",
      "Object obj1 deleted\n",
      "Object obj2 deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Handled Circular reference\n",
    "import gc\n",
    "\n",
    "class MyObject:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        print(f\"Object {self.name} created\")\n",
    "\n",
    "    def __del__(self):\n",
    "        print(f\"Object {self.name} deleted\")\n",
    "\n",
    "# Create circular reference\n",
    "obj1 = MyObject(\"obj1\")\n",
    "obj2 = MyObject(\"obj2\")\n",
    "obj1.ref = obj2\n",
    "obj2.ref = obj1\n",
    "\n",
    "del obj1\n",
    "del obj2\n",
    "\n",
    "## Manually trigger the garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "## Generators For Memory Efficiency\n",
    "#Generators allow you to produce items one at a time, using memory efficiently by only keeping one item in memory at a time.\n",
    "\n",
    "def generate_numbers(n):\n",
    "    for i in range(n):\n",
    "        yield i\n",
    "\n",
    "## using the generator\n",
    "for num in generate_numbers(100000):\n",
    "    print(num)\n",
    "    if num>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiling Memory USage with tracemalloc\n",
    "# Profiling memory usage involves tracking how much memory your program is using, identifying memory leaks, and understanding memory consumption patterns. This helps in optimizing the program to use memory more efficiently.\n",
    "\n",
    "# tracemalloc is a module in Python that helps in tracing memory allocations. It allows you to take snapshots of memory usage and compare them to identify where memory is being allocated and how much.\n",
    "\n",
    "# Here is the updated code with the comment:\n",
    "\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "def create_list():\n",
    "    return [i for i in range(10000)]\n",
    "\n",
    "def main():\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    create_list()\n",
    "    \n",
    "    snapshot = tracemalloc.take_snapshot()\n",
    "    top_stats = snapshot.statistics('lineno')\n",
    "    \n",
    "    print(\"[ Top 10 ]\")\n",
    "    for stat in top_stats[::]:\n",
    "        print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Top 10 ]\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:353: size=3084 B, count=34, average=91 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\compilerop.py:174: size=1831 B, count=21, average=87 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\traitlets\\traitlets.py:731: size=1392 B, count=23, average=61 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\codeop.py:126: size=1325 B, count=11, average=120 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\traitlets\\traitlets.py:1543: size=1247 B, count=21, average=59 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:100: size=1241 B, count=8, average=155 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\traitlets\\traitlets.py:1514: size=1080 B, count=9, average=120 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\zmq\\sugar\\socket.py:805: size=1056 B, count=6, average=176 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\zmq\\sugar\\attrsettr.py:45: size=987 B, count=21, average=47 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:782: size=914 B, count=2, average=457 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\jsonutil.py:111: size=900 B, count=18, average=50 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:1057: size=829 B, count=6, average=138 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\compilerop.py:86: size=787 B, count=10, average=79 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:293: size=760 B, count=2, average=380 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:346: size=664 B, count=10, average=66 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:276: size=592 B, count=6, average=99 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3104: size=520 B, count=4, average=130 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:287: size=448 B, count=5, average=90 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:775: size=424 B, count=2, average=212 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\jsonutil.py:75: size=416 B, count=2, average=208 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: size=376 B, count=1, average=376 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:836: size=360 B, count=5, average=72 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py:362: size=360 B, count=1, average=360 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:815: size=312 B, count=3, average=104 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3517: size=296 B, count=1, average=296 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py:88: size=288 B, count=4, average=72 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\tornado\\queues.py:248: size=288 B, count=2, average=144 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:290: size=280 B, count=2, average=140 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:527: size=272 B, count=4, average=68 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:534: size=264 B, count=1, average=264 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:435: size=248 B, count=1, average=248 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1622: size=240 B, count=1, average=240 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:545: size=232 B, count=1, average=232 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py:549: size=224 B, count=3, average=75 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3223: size=208 B, count=4, average=52 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py:235: size=208 B, count=3, average=69 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:1085: size=208 B, count=1, average=208 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:770: size=208 B, count=1, average=208 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\compiler.py:91: size=207 B, count=2, average=104 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:418: size=200 B, count=5, average=40 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:688: size=194 B, count=2, average=97 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:750: size=192 B, count=2, average=96 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\zmq\\sugar\\socket.py:802: size=184 B, count=2, average=92 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3224: size=176 B, count=4, average=44 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1182: size=173 B, count=2, average=86 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:570: size=160 B, count=1, average=160 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: size=160 B, count=1, average=160 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3493: size=160 B, count=1, average=160 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3237: size=160 B, count=1, average=160 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_19992\\3857728548.py:14: size=160 B, count=1, average=160 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_19992\\3857728548.py:11: size=160 B, count=1, average=160 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:394: size=160 B, count=1, average=160 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:387: size=160 B, count=1, average=160 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:381: size=160 B, count=1, average=160 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:722: size=144 B, count=2, average=72 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:449: size=144 B, count=1, average=144 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:318: size=120 B, count=2, average=60 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:135: size=120 B, count=2, average=60 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:266: size=120 B, count=1, average=120 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:989: size=111 B, count=2, average=56 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\dateutil\\parser\\isoparser.py:146: size=96 B, count=2, average=48 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3375: size=96 B, count=2, average=48 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:805: size=90 B, count=2, average=45 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:851: size=72 B, count=1, average=72 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:834: size=72 B, count=1, average=72 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:292: size=72 B, count=1, average=72 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:288: size=72 B, count=1, average=72 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:283: size=72 B, count=1, average=72 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:282: size=72 B, count=1, average=72 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3509: size=64 B, count=2, average=32 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1971: size=64 B, count=2, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\zmq\\sugar\\socket.py:806: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py:639: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:1053: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py:384: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py:383: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py:294: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py:291: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\decorator.py:232: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3566: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3303: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3300: size=64 B, count=1, average=64 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:200: size=48 B, count=1, average=48 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3505: size=48 B, count=1, average=48 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:835: size=48 B, count=1, average=48 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:330: size=48 B, count=1, average=48 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\prefilter.py:317: size=47 B, count=1, average=47 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\jupyter_client\\session.py:996: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:637: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\ipykernel\\iostream.py:168: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3503: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\history.py:793: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\compilerop.py:192: size=32 B, count=1, average=32 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:421: size=32 B, count=1, average=32 B\n",
      "C:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1960: size=32 B, count=1, average=32 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\compilerop.py:172: size=28 B, count=1, average=28 B\n",
      "h:\\My Drive\\IMPORTANT\\Data science and machine learning\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3485: size=8 B, count=1, average=8 B\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
